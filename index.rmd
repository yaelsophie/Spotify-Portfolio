---
title: "Storyboard"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
---


```{r, results = 'hide'}
library(tidyverse)
library(tidymodels)
library(ggdendro)
library(heatmaply)
library(spotifyr)
library(dplyr)
library(ggplot2)
library(compmus)

```

```{r}

patrick <- get_playlist_audio_features("", "6wlIYjCXIv8vzRWxn8Ee9I")

```

### Introduction

Analyzing the development of the music of Patrick Watson over the years


There's one artist that has amazed me the most during my own development in music. For the past several years I've began to love R&B and Neo Soul, and Spotify has guessed these genres quite correctly as seen in my Spotify Wrapped, but this one artist stands out from these genres, and with whom I can connect the most during listening to his songs is Patrick Watson.


Patrick Watson composes music in a different genre, namely chamber music but also fusions between classical music and indierock. Over the years, he developed his own style and although these aformentioned genres do make sense, there's something to his music that makes that I can't simply classify his music into a specific genre. This can be due to the various styles he showed in his albums, or the strong emotional feeling I get while listening to his music that makes his music more distinctive from other chamber/indierock music.


For now I'm going to step away from the genre classification, but I'm going to examine what kind of developments or changes he made while making various albums over the years. This way, I'm going to take a step away from the emotional meanings in his music, analyse what happens on a computational and numerical matter, and then link this back to emotional circumstances again.


I'm not only going to compare the albums to each other, but also what happens within an album. Does it start off with a somehow reserved energy, and will it evolve to a more energetic happining? For example, with the album 'Wave', Patrick Watson explained in an interview that he went through some grief while writing songs for the album. The beginning of the album feels darker coloured and heavier, while the second half of the album feels more uplifting and bright. Is this also showable in some analytics from the songs with the help of looking at features like BPM and pitch?


Furthermore, Patrick Watson also mentioned in an interview that he chose to sing lower pitched in his latest album 'Wave'. Is this actually true compared to his other albums, or is it just something he intented to do, but remains more or less the same?
Link to interview: https://www.youtube.com/watch?v=UqliZIpHWU0


I've made a Spotify playlist including 6 albums. One is left out called 'The Ninth life of louis drax (original motion picture soundtrack)' because it doesn't contain any vocals and is composed for a movie. The playlist contains the following albums: 'Wave', 'Love Songs for Robots', 'Adventures in your Own Backyard', 'Wooden Arms', 'Close to Paradise' and 'Just another Ordinary Day', resulting in a album consisting of 66 songs. (Possibly I will also include EP songs, but that is something I'll decide later on.)
Link to playlist: https://open.spotify.com/playlist/6wlIYjCXIv8vzRWxn8Ee9I?si=732901d3b2924598


Typical songs Lighthouse - Patrick Watson Typical because he first uses instruments he is comfortable with, i.e. piano, and uses a height in pitch he uses very often. Lately the song emerges into a heavy instrumentalised song and stronger vocals, where a lot is happening. This is also very tipical for a Patrick Watsong song.


Atypical songs The Storm - Patrick Watson Atypical because you directly get the feeling that it comes close to a country song while he often doesn't show this style in his music. My question here is, why does it feel like a country song instead of just another song from Patrick Watson?
More typical and atypical songs will follow.


The albums Wave and Close to Paradise have similar scores of speechiness that are close together, they don't differentiate a lot. While other albums differentiate a lot from speechiness within the album itself.

I've excluded the album 'the ninth life of louis drax (original motion picture soundtrack)' because it is a cinematic album and doesn't show a lot of vocal use

### Visualisation I

```{r}
patrick %>%
  ggplot(aes(x = mode)) +
  geom_histogram(binwidth = 0.5) 

```

***

My plan is to make a Visualisation per album to see the difference in mode. Is it a song made in major or minor tuning? 
I hope to see a increase or decrease in the number of songs per album that are defined as major or minor, and link this to his life in time of making that album.


### Visualisation II

```{r}
patrick %>%
  ggplot(aes(x = speechiness, y = track.album.name)) +
  geom_point() 

```


### Self Similarity matrix with chroma (pitches)

```{r}

library(tidyverse)
library(spotifyr)
library(dplyr)
library(ggplot2)
library(compmus)

wave <-
  get_tidy_audio_analysis("6J0qtvtsAItRdv1Ba4ZdFS") %>% # Change URI.
  compmus_align(beats, segments) %>%                     # Change `bars`
  select(beats) %>%                                      #   in all three
  unnest(beats) %>%                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

wave %>%
  compmus_self_similarity(pitches, "cosine") %>% 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(title = "Selfsimilarity Matrix with Chroma", x = "", y = "")
```

***

This is again a plot for the song 'Wave', where, instead of the first Self Similarity Matrix with timbre, I've used pitches to show the Self Similarity with Chroma. 
I've come to a realisation that the song has a lot of similar use of pitch/chroma during the entire song, and this is also shown in the Chromagram plot where you can actually see that co2 is often used. 


### Chroma

```{r}

library(tidyverse)
library(spotifyr)
library(dplyr)
library(ggplot2)
library(compmus)

wave <-
  get_tidy_audio_analysis("6J0qtvtsAItRdv1Ba4ZdFS") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

likeyou <-
  get_tidy_audio_analysis("3KBxdqIInwuU5aIEeLmBPp") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

compmus_long_distance(
  wave %>% mutate(pitches = map(pitches, compmus_normalise, "chebyshev")),
  likeyou %>% mutate(pitches = map(pitches, compmus_normalise, "chebyshev")),
  feature = pitches,
  method = "euclidean"
) %>%
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_equal() +
  labs(x = "Wave", y = "Man Like You") +
  theme_minimal() +
  scale_fill_viridis_c(guide = NULL)
```

***

Here, you see a Chroma plot for two different songs, 'Man Like You' and 'Wave'. I still have to manage to make the plot more clear as you can see. The plot is now not easily readable and interpretable. 


### Self similarity matrix with Timbre

```{r}

library(tidyverse)
library(spotifyr)
library(dplyr)
library(ggplot2)
library(compmus)

wave <-
  get_tidy_audio_analysis("6J0qtvtsAItRdv1Ba4ZdFS") %>% # Change URI.
  compmus_align(beats, segments) %>%                     # Change `bars`
  select(beats) %>%                                      #   in all three
  unnest(beats) %>%                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

wave %>%
  compmus_self_similarity(timbre, "cosine") %>% 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_classic() +
  labs(title = "Selfsimilarity Matrix", x = "", y = "")
```

***

In this plot, patterns in timbre is being shown for the song 'Wave'. It is clear that there are 4 different segments in the song as you can see in the plot. This is surprising for me, because I haven't really paid attention to this before and thought that there were more than 4 different segments. This could of course be true, because my perception of the song is different from that of a algorithm that sorts out the patterns based on timbre. But it is still interesting to see this outcome. 


### Tempogram

```{r}
wave <- get_tidy_audio_analysis("6J0qtvtsAItRdv1Ba4ZdFS")

wave %>%
  tempogram(window_size = 8, hop_size = 1, cyclic = TRUE) %>%
  ggplot(aes(x = time, y = bpm, fill = power)) +
  geom_raster() +
  scale_fill_viridis_c(guide = "none") +
  labs(x = "Time (s)", y = "Tempo (BPM)") +
  theme_classic()
```

***

Here, I've plotted a plot (tempogram) for the song 'The Wave' - Patrick Watson. The song starts off with an ensemble of classical instruments, mostly strings, that uses a lot of notes with a long duration that create a sort of 'Wave', so to say. Because of those long notes, a specific and consistent tempo is not easily found. Also, with the human ear, I found it not easily recognizable. Therefore, in the tempogram, you don't see an obvious line seconds before second 50. After this, vocals will join and the tempo is a little easier to find. You can find a line just above 140 BPM from 45 to 175 seconds, After 175 seconds, a lot of string instruments will return and the tempo is again more difficult to find. 


### Dendrogram

```{r}
wave <- get_tidy_audio_analysis("6J0qtvtsAItRdv1Ba4ZdFS")
```

```{r}
halloween <-
  get_playlist_audio_features("", "6wlIYjCXIv8vzRWxn8Ee9I") %>%
  add_audio_analysis() %>%
  mutate(
    segments = map2(segments, key, compmus_c_transpose),
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      ),
    timbre =
      map(
        segments,
        compmus_summarise, timbre,
        method = "mean"
      )
  ) %>%
  mutate(pitches = map(pitches, compmus_normalise, "clr")) %>%
  mutate_at(vars(pitches, timbre), map, bind_rows) %>%
  unnest(cols = c(pitches, timbre))
```

```{r}
halloween_juice <-
  recipe(
    track.name ~
      energy +
      loudness +
      acousticness +
      instrumentalness +
      valence +
      tempo,
    data = halloween
  ) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors()) %>% 
  # step_range(all_predictors()) %>% 
  prep(halloween %>% mutate(track.name = str_trunc(track.name, 20))) %>%
  juice() %>%
  column_to_rownames("track.name")
```
```{r}
halloween_dist <- dist(halloween_juice, method = "manhattan")
```

```{r}
halloween_dist %>% 
  hclust(method = "complete") %>% # Try single, average, and complete.
  dendro_data() %>%
  ggdendrogram()
```

***

Here, I've plotted a dendrogram and in the next tab a heatmap. I want to make the dendrogram more clear by adding colour to different albums, so that it becomes easier to recognize what song belongs to which album and why the dendrogram chose to put certain songs more near each other than others. The features my dendrogram did its calculations on were: energy, loudness, acousticness, instrumentalness, valence and tempo. I've excluded features like speechiness, because in the audio analysis of the album I've seen that the values of this feature range in a very narrow matter.


### Keygram

```{r}

library(tidyverse)
library(spotifyr)
library(dplyr)
library(ggplot2)
library(compmus)

circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

lighthouse <-
  get_tidy_audio_analysis("4vqp9GaO7RVkinyrYY5W6R") %>%
  compmus_align(sections, segments) %>%
  select(sections) %>%
  unnest(sections) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

lighthouse %>% 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```

***

This is a Keygram for the song Lighthouse - Patrick Watson. 
The song starts off very quiet and peaceful, so to say. The pitches you hear, come in a repetitive pattern, which you can see in the keygram from second 0 to around 150 seconds. From 150 seconds onwards, multiple instruments such as trumpets and a string ensemble will join, and a different pitch pattern will show up in the keygram. At the end of the song you will hear that it returns back to the beginning of the song, with a minimal number of instruments. You can clearly see this in the keygram. 


### Chordogram

```{r}

library(tidyverse)
library(spotifyr)
library(dplyr)
library(ggplot2)
library(compmus)

circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

lighthouse <-
  get_tidy_audio_analysis("4vqp9GaO7RVkinyrYY5W6R") %>%
  compmus_align(sections, segments) %>%
  select(sections) %>%
  unnest(sections) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )

lighthouse %>% 
  compmus_match_pitch_template(
    chord_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")
```

***

In the Chordogram, you see less of a distinction of patterns. I think this is because the chords stay more or less the same during the whole song, but because other instruments will join from 150 seconds, you will see a pattern in the keygram. 
At around 140 seconds in you see a different pattern than before. This is where the instruments join in, and I guess that a different pattern is shown because other instruments play other leading notes and therefore the algorithm might pick up different chords.
Just before 200 seconds, you also see a distinction with respect to the first part. You can listen to the song and hear that the regular chord pattern will return.


### Cepstogram

```{r}

library(tidyverse)
library(spotifyr)
library(dplyr)
library(ggplot2)
library(compmus)

wave <-
  get_tidy_audio_analysis("6J0qtvtsAItRdv1Ba4ZdFS") %>% # Change URI.
  compmus_align(beats, segments) %>%                     # Change `bars`
  select(beats) %>%                                      #   in all three
  unnest(beats) %>%                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

wave %>%
  compmus_gather_timbre() %>%
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(title = "Chromagram", x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +                              
  theme_classic()
```

***

This is a cepstogram based on the song 'Wave'.
It is seen that he used the tone c02 a lot. If you listen to the song, this is also quiet easy to hear and recognise.

I found that while making these plots, using Euclidean normalisation worked the best. I saw the most contrast in these plots and patterns where easily seen. 


### Conclusion

For now, I've plotted a few chromagrams and self similarity matrices, to create an insight into what is happening in the song 'Wave'. In the future, I'll try to make a plot for 2 different songs from 2 different albums that are very typical and then compare them. Eventually I hope to see what kind of developments Patrick Watson made during his carriere.

